{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_Neural Data Analysis_\n",
    "\n",
    "Lecturer: PD Dr. Philipp Berens, Dr. Alexander Ecker\n",
    "\n",
    "Tutors: Sophie Laturnus, Santiago Cadena\n",
    "\n",
    "Summer term 2018\n",
    "\n",
    "Due date: 24.7.18\n",
    "\n",
    "# Exercise sheet 8\n",
    "\n",
    "In this exercise we are going to fit a latent variable model (Poisson GPFA) to both toy data and real data from monkey primary visual cortex.\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "1. Clone the poisson-GPFA repository from https://github.com/mackelab/poisson-gpfa and make sure that you have a directory ```../funs/```  in the folder with this notebook. The toolbox contains an implementation of the EM algorithm to fit the poisson-gpfa. For the desciption of the algorithm please refer to https://hooram.xyz/projects.html \n",
    "\n",
    "2. Download the data file ```nda_ex_8_data.mat``` from ILIAS and save it in a subfolder ```../data/```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import funs.util as util\n",
    "import funs.engine as engine\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datamanager\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pdb\n",
    "import numpy.matlib\n",
    "from statsmodels.stats.moment_helpers import cov2corr\n",
    "\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "sns.set_context('paper')\n",
    "sns.set(rc={'image.cmap': 'bwr'})\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Generate some toy data to test the poisson-GPFA code\n",
    "\n",
    "We start by verifying our code on toy data. The cell below contains code to generate data for 30 neurons, 100 trials (1000 ms each) and 50ms bin size. The neurons' firing rate $\\lambda_k$ is assumed to be a constant $d_k$ modulated by a one-dimensional latent state $x$, which is drawn from a Gaussian process:\n",
    "\n",
    "$\\lambda_k = \\exp(c_kx + d_k)$\n",
    "\n",
    "Each neuron's weight $c_k$ is drawn randomly from a normal distribution and spike counts are sampled form a Poisson distribution with rate $\\lambda_k$.\n",
    "\n",
    "Your task is to fit a Poisson GPFA model with one latent variable to this data (see `engine.PPGPFAfit`).\n",
    "\n",
    "*Grading: 3 pts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize random number generator\n",
    "#np.random.seed(10)\n",
    "# Specify dataset & fitting parameters\n",
    "xdim      =1#latent dimensionality to fit\n",
    "ydim      = 30\t\t #number of neurons in the dataset\n",
    "numTrials = 100\t\t\n",
    "trialDur  = 1000 # in ms\n",
    "binSize   = 50\t # in ms\n",
    "maxEMiter = 100\t\t\n",
    "dOffset   = 1.5\t # controls firing rate\n",
    "\n",
    "# Sample from the model (make a toy dataset)\n",
    "training_set  = util.dataset(\n",
    "    seed      =345533,\n",
    "    xdim      = xdim,\n",
    "\tydim      = ydim,\n",
    "\tnumTrials = numTrials,\n",
    "\ttrialDur  = trialDur,\n",
    "\tbinSize   = binSize,\n",
    "\tdOffset   = dOffset,\n",
    "\tfixTau \t  = True, \n",
    "\tfixedTau  = np.array([0.2]),\n",
    "\tdrawSameX = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize parameters using Poisson-PCA\n",
    "initParams = util.initializeParams(xdim, ydim, training_set)\n",
    "\n",
    "# fit the model\n",
    "fitToy = engine.PPGPFAfit(\n",
    "    #todo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some useful functions\n",
    "def allTrialsState(fit,p):\n",
    "    \"\"\"Reshape the latent signal and the spike counts\"\"\"\n",
    "    x = np.zeros([p,0])\n",
    "    for i in range(len(fit.infRes['post_mean'])):\n",
    "        x = np.concatenate((x,fit.infRes['post_mean'][i]),axis=1)\n",
    "    return x\n",
    "\n",
    "def allTrialsX(training_set):\n",
    "    \"\"\"Reshape the ground truth \n",
    "    latent signal and the spike counts\"\"\"\n",
    "    x_gt = np.array([])\n",
    "    for i in range(len(training_set.data)):\n",
    "        x_gt =  np.concatenate((x_gt,training_set.data[i]['X'][0]),axis = 0)\n",
    "    return x_gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the ground truth vs. inferred model\n",
    "Verify your fit by plotting both ground truth and inferred parameters for:\n",
    "1. weights C\n",
    "2. biases d\n",
    "3. spike counts covariance matrix\n",
    "4. latent state x \n",
    "\n",
    "Note that the sign of fitted latent state and its weights are ambiguous (you can flip both without changing the model). Make sure you correct the sign for the plot if it does not match the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# All trials latent state vector\n",
    "x = allTrialsState(fitToy,1)\n",
    "x_gt = getOriginalX(training_set)\n",
    "\n",
    "# fill in plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Fit GPFA model to real data. \n",
    "\n",
    "We now fit the model to real data and cross-validate over the dimensionality of the latent variable.\n",
    "\n",
    "*Grading: 2 pts*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "The cell below implements loading the data and encapsulates it into a class that matches the interface of the Poisson GPFA engine. You don't need to do anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EckerDataset():\n",
    "    \"\"\"Loosy class\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        path = 'data/task8_data.mat',\n",
    "        subject_id=0,\n",
    "        ydim = 55,\n",
    "        trialDur = 400,\n",
    "        binSize = 20,\n",
    "        numTrials = 100,\n",
    "        ydimData = False,\n",
    "        numTrData = True):\n",
    "        \n",
    "        T = binSize#int(trialDur/binSize)\n",
    "        matdat = sio.loadmat(path)\n",
    "        self.matdat = matdat\n",
    "        data = []\n",
    "        trial_durs = []\n",
    "        for trial_id in range(numTrials):\n",
    "            trial_time =matdat['spikeTimes'][:,trial_id][0] \n",
    "            trial_big_time = np.min(trial_time)\n",
    "            trial_end_time = np.max(trial_time)\n",
    "            trial_durs.append(trial_end_time - trial_big_time)         \n",
    "        for trial_id in range(numTrials):\n",
    "            Y = []\n",
    "            spike_time = []\n",
    "            data.append({\n",
    "                'Y': matdat['spikeCounts'][:,:,trial_id],\n",
    "                'spike_time': matdat['spikeTimes'][:,trial_id]})\n",
    "        self.T = T\n",
    "        self.trial_durs = trial_durs    \n",
    "        self.data = data\n",
    "        self.trialDur = trialDur\n",
    "        self.binSize = binSize\n",
    "        self.numTrials = numTrials\n",
    "        self.ydim = ydim              \n",
    "        util.dataset.getMeanAndVariance(self)\n",
    "        util.dataset.getAvgFiringRate(self)\n",
    "        util.dataset.getAllRaster(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '../data/nda_ex_8_data.mat'\n",
    "data = EckerDataset(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Poisson GPFA models and perform model comparison\n",
    "\n",
    "Split the data into 80 trials used for training and 20 trials held out for performing model comparison. On the training set, fit models using one to five latent variables. Compute the performance of each model on the held-out test set.\n",
    "\n",
    "Hint: You can use the `crossValidation` function in the Poisson GPFA package.\n",
    "\n",
    "Optional: The `crossValidation` function computes the mean-squared error on the test set, which is not ideal. The predictive log-likelihood under the Poisson model would be a better measure, which you are welcome to compute instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the test error\n",
    "\n",
    "Make a plot of the test error for the five different models. As a baseline, please also include the test error of a model without a latent variable. This is essentially the mean-squared error of a constant rate model (or Poisson likelihood if you did the optional part above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Visualization: population rasters and latent state. Use the model with a single latent state. \n",
    "\n",
    "Create a raster plot where you show for each trial the spikes of all neurons as well as the trajectory of the latent state `x`. Sort the neurons by their weights `c_k`. Plot only the first 20 trials.\n",
    "\n",
    "*Grading: 2 pts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your plot here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Visualization of covariance matrix.\n",
    "\n",
    "Plot (a) the noise covariance matrix as well as its approximation using (b) one and (c) five latent variable(s). Use the analytical solution for the covariance matrix of the approximation*. Note that the solution is essentially the mean and covariance of the [log-normal distribution](https://en.wikipedia.org/wiki/Log-normal_distribution).\n",
    "\n",
    "$ \\mu = \\exp(\\frac{1}{2} \\text{ diag}(CC^T)+d)$\n",
    "\n",
    "$ \\text{Cov}= \\mu\\mu^T \\exp(CC^T)+\\text{ diag}(\\mu) - \\mu\\mu^T$ \n",
    "\n",
    "*[Krumin, M., and Shoham, S. (2009). Generation of Spike Trains with Controlled Auto- and Cross-Correlation Functions. Neural Computation 21, 1642–1664](http://www.mitpressjournals.org/doi/10.1162/neco.2009.08-08-847).\n",
    "\n",
    "*Grading: 3 pts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your plot here\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
