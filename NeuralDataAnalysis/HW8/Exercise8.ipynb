{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Neural Data Analysis_\n",
    "\n",
    "Lecturer: Prof. Dr. Philipp Berens, Dr. Alexander Ecker\n",
    "\n",
    "Tutors: Sarah Strauss, Santiago Cadena\n",
    "\n",
    "Summer term 2019\n",
    "\n",
    "Due date: 2019-07-09, 9am \n",
    "\n",
    "Names: Guillem Boada, Peter El-Jiz, Ulzii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning \n",
    "\n",
    "The code will not run with the default https://github.com/mackelab/poisson-gpfa. The library has been forked and heavy modifications have been performed. It can be found at https://github.com/peterjiz/poisson-gpfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise sheet 8\n",
    "\n",
    "In this exercise we are going to fit a latent variable model (Poisson GPFA) to both toy data and real data from monkey primary visual cortex.\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "1. Clone the poisson-GPFA repository from https://github.com/mackelab/poisson-gpfa and make sure that you have a directory ```../funs/```  in the folder with this notebook. The toolbox contains an implementation of the EM algorithm to fit the poisson-gpfa. For the desciption of the algorithm please refer to https://hooram.xyz/projects.html \n",
    "\n",
    "2. Download the data file ```nda_ex_8_data.mat``` from ILIAS and save it in a subfolder ```../data/```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C-Style Print Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def sprintf(format, *values):\n",
    "    s = format % values \n",
    "    return s\n",
    "\n",
    "def printf(format, *values):\n",
    "    sys.stdout.write(format % values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'funs.cvalidate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-99ace81c25d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfuns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfuns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfuns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvalidate\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcvalidate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'funs.cvalidate'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import funs.util as util\n",
    "import funs.engine as engine \n",
    "import funs.cvalidate as cvalidate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datamanager\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pdb\n",
    "import numpy.matlib\n",
    "import pandas as pd\n",
    "from statsmodels.stats.moment_helpers import cov2corr\n",
    "\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "sns.set_context('paper')\n",
    "sns.set(rc={'image.cmap': 'bwr'})\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(util)\n",
    "importlib.reload(engine)\n",
    "importlib.reload(cvalidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Generate some toy data to test the poisson-GPFA code\n",
    "\n",
    "We start by verifying our code on toy data. The cell below contains code to generate data for 30 neurons, 100 trials (1000 ms each) and 50ms bin size. The neurons' firing rate $\\lambda_k$ is assumed to be a constant $d_k$ modulated by a one-dimensional latent state $x$, which is drawn from a Gaussian process:\n",
    "\n",
    "$\\lambda_k = \\exp(c_kx + d_k)$\n",
    "\n",
    "Each neuron's weight $c_k$ is drawn randomly from a normal distribution and spike counts are sampled from a Poisson distribution with rate $\\lambda_k$.\n",
    "\n",
    "Your task is to fit a Poisson GPFA model with one latent variable to this data (see `engine.PPGPFAfit`).\n",
    "\n",
    "*Grading: 3 pts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize random number generator\n",
    "#np.random.seed(10)\n",
    "# Specify dataset & fitting parameters\n",
    "xdim      =1#latent dimensionality to fit\n",
    "ydim      = 30\t\t #number of neurons in the dataset\n",
    "numTrials = 100\t\t\n",
    "trialDur  = 1000 # in ms\n",
    "binSize   = 50\t # in ms\n",
    "maxEMiter = 100\t\t\n",
    "dOffset   = 1.5\t # controls firing rate\n",
    "\n",
    "# Sample from the model (make a toy dataset)\n",
    "training_set  = util.dataset(\n",
    "    seed      =345533,\n",
    "    xdim      = xdim,\n",
    "\tydim      = ydim,\n",
    "\tnumTrials = numTrials,\n",
    "\ttrialDur  = trialDur,\n",
    "\tbinSize   = binSize,\n",
    "\tdOffset   = dOffset,\n",
    "\tfixTau \t  = True, \n",
    "\tfixedTau  = np.array([0.2]),\n",
    "\tdrawSameX = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters using Poisson-PCA\n",
    "initParams = util.initializeParams(xdim, ydim, training_set)\n",
    "\n",
    "# fit the model\n",
    "fitToy = engine.PPGPFAfit(\n",
    "    #todo\n",
    "\texperiment \t\t= training_set,\n",
    "\tinitParams \t\t= initParams,\n",
    "\tinferenceMethod = 'laplace',\n",
    "\tEMmode \t\t\t= 'Batch',\n",
    "\tmaxEMiter \t\t= maxEMiter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some useful functions\n",
    "def allTrialsState(fit,p):\n",
    "    \"\"\"Reshape the latent signal and the spike counts\"\"\"\n",
    "    x = np.zeros([p,0])\n",
    "    for i in range(len(fit.infRes['post_mean'])):\n",
    "        x = np.concatenate((x,fit.infRes['post_mean'][i]),axis=1)\n",
    "    return x\n",
    "\n",
    "def allTrialsX(training_set):\n",
    "    \"\"\"Reshape the ground truth \n",
    "    latent signal and the spike counts\"\"\"\n",
    "    x_gt = np.array([])\n",
    "    for i in range(len(training_set.data)):\n",
    "        x_gt =  np.concatenate((x_gt,training_set.data[i]['X'][0]),axis = 0)\n",
    "    return x_gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the ground truth vs. inferred model\n",
    "Verify your fit by plotting both ground truth and inferred parameters for:\n",
    "1. weights C\n",
    "2. biases d\n",
    "3. spike counts covariance matrix\n",
    "4. latent state x \n",
    "\n",
    "Note that the sign of fitted latent state and its weights are ambiguous (you can flip both without changing the model). Make sure you correct the sign for the plot if it does not match the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# All trials latent state vector\n",
    "x = allTrialsState(fitToy,1)\n",
    "x_gt = allTrialsX(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_gt = training_set.params['C']\n",
    "d_gt = training_set.params['d']\n",
    "tau_gt = training_set.params['tau']\n",
    "\n",
    "C_est = fitToy.optimParams['C']\n",
    "d_est = fitToy.optimParams['d']\n",
    "tau_est = fitToy.optimParams['tau']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ground truth weights C, biases d and K(tau)\n",
    "plt.suptitle('Ground truth weights C and biases d', fontsize=17)\n",
    "training_set.plotParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot fitted weights C, biases d and K(tau)\n",
    "plt.suptitle('Inferred weights C and biases d',fontsize=17)\n",
    "fitToy.plotOptimParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 12))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title(\"Ground Truth C\")\n",
    "plt.plot(C_gt)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title(\"Estimated C\")\n",
    "plt.plot(-C_est)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.title(\"Ground Truth d\")\n",
    "plt.plot(d_gt)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.title(\"Estimated d\")\n",
    "plt.plot(d_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spike counts covariance matrix\n",
    "fitToy.plotCovAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Ground Truth Cov[y]\")\n",
    "plt.imshow(fitToy.E_yy_true_params)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Estimated Cov[y]\")\n",
    "plt.imshow(fitToy.E_yy_optim_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $x$ is of dimensions $pT * 1$; to get all times for a trial, we need to index from $trial * (T=\\text{max time bin})$ to $(trial+1)*(T=\\text{max time bin})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in plot here\n",
    "plt.figure(figsize=(24, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Ground Truth x\")\n",
    "plt.plot(x_gt)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Estimated x\")\n",
    "plt.plot(x.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in plot \n",
    "plt.figure(figsize=(24,12))\n",
    "plt.suptitle('Ground Truth & Estimated x',fontsize=17)\n",
    "for trial in range(0, 10):\n",
    "    plt.subplot(2, 5, trial+1)\n",
    "    plt.plot(x_gt[trial*20:(trial+1)*20],label='Ground Truth')\n",
    "    plt.plot(-x.T[trial*20:(trial+1)*20],'r',label='Estimated')\n",
    "    plt.title(sprintf(\"Trial %i\", trial+1))\n",
    "    plt.xlabel('Time Bins')\n",
    "    plt.ylabel('Latent x')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Fit GPFA model to real data. \n",
    "\n",
    "We now fit the model to real data and cross-validate over the dimensionality of the latent variable.\n",
    "\n",
    "*Grading: 2 pts*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "The cell below implements loading the data and encapsulates it into a class that matches the interface of the Poisson GPFA engine. You don't need to do anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EckerDataset():\n",
    "    \"\"\"Loosy class\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        path = 'data/task8_data.mat',\n",
    "        subject_id=0,\n",
    "        ydim = 55,\n",
    "        trialDur = 400,\n",
    "        binSize = 20,\n",
    "        numTrials = 100,\n",
    "        ydimData = False,\n",
    "        numTrData = True):\n",
    "        \n",
    "        T = binSize#int(trialDur/binSize)\n",
    "        matdat = sio.loadmat(path)\n",
    "        self.matdat = matdat\n",
    "        data = []\n",
    "        trial_durs = []\n",
    "        for trial_id in range(numTrials):\n",
    "            trial_time =matdat['spikeTimes'][:,trial_id][0] \n",
    "            trial_big_time = np.min(trial_time)\n",
    "            trial_end_time = np.max(trial_time)\n",
    "            trial_durs.append(trial_end_time - trial_big_time)         \n",
    "        for trial_id in range(numTrials):\n",
    "            Y = []\n",
    "            spike_time = []\n",
    "            data.append({\n",
    "                'Y': matdat['spikeCounts'][:,:,trial_id],\n",
    "                'spike_time': matdat['spikeTimes'][:,trial_id]})\n",
    "        self.T = T\n",
    "        self.trial_durs = trial_durs    \n",
    "        self.data = data\n",
    "        self.trialDur = trialDur\n",
    "        self.binSize = binSize\n",
    "        self.numTrials = numTrials\n",
    "        self.ydim = ydim              \n",
    "        util.dataset.getMeanAndVariance(self)\n",
    "        util.dataset.getAvgFiringRate(self)\n",
    "        util.dataset.getAllRaster(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/nda_ex_8_data.mat'\n",
    "data = EckerDataset(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Poisson GPFA models and perform model comparison\n",
    "\n",
    "Split the data into 80 trials used for training and 20 trials held out for performing model comparison. On the training set, fit models using one to five latent variables. Compute the performance of each model on the held-out test set.\n",
    "\n",
    "Hint: You can use the `crossValidation` function in the Poisson GPFA package.\n",
    "\n",
    "Optional: The `crossValidation` function computes the mean-squared error on the test set, which is not ideal. The predictive log-likelihood under the Poisson model would be a better measure, which you are welcome to compute instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as op \n",
    "import funs.inference as inference \n",
    "\n",
    "def predictiveLogLikeErrorFunc(params, experiment):\n",
    "    '''\n",
    "    Performs leave-one-out prediction. \n",
    "    '''\n",
    "    ydim, xdim = np.shape(params['C'])\n",
    "    print('Performing cross validation with predictiveLogLikeErrorFunc...')\n",
    "    y_pred_mode_all = []\n",
    "    pred_err_mode = 0\n",
    "    for tr in range(experiment.numTrials):\n",
    "        y_pred_mode_tr = []\n",
    "        for nrn in range(experiment.ydim):\n",
    "            # Make params without neuron# nrn\n",
    "            CwoNrn = np.delete(params['C'],nrn,0)\n",
    "            dwoNrn = np.delete(params['d'],nrn,0)\n",
    "            paramsSplit = {'C':CwoNrn, 'd':dwoNrn, 'tau':params['tau']}\n",
    "\n",
    "            # Make params with only neuron# nrn\n",
    "            C_nrn = params['C'][nrn]\n",
    "            d_nrn = params['d'][nrn]\n",
    "\n",
    "            # Make params big\n",
    "            C_big, d_big = util.makeCd_big(paramsSplit,experiment.T)\n",
    "            K_big, K = util.makeK_big(paramsSplit, experiment.trialDur, experiment.binSize)\n",
    "            K_bigInv = np.linalg.inv(K_big)\n",
    "\n",
    "            # Make data without neuron# nrn\n",
    "            y = np.delete(experiment.data[tr]['Y'],nrn,0)\n",
    "            ybar = np.ndarray.flatten(np.reshape(y, (experiment.ydim-1)*experiment.T))\n",
    "\n",
    "            xInit = np.ndarray.flatten(np.zeros([xdim*experiment.T,1]))\n",
    "            res = op.fmin_ncg(\n",
    "                f = inference.negLogPosteriorUnNorm,\n",
    "                x0 = xInit,\n",
    "                fprime = inference.negLogPosteriorUnNorm_grad,\n",
    "                fhess = inference.negLogPosteriorUnNorm_hess,\n",
    "                args = (ybar, C_big, d_big, K_bigInv, xdim, experiment.ydim-1),\n",
    "                disp = False,\n",
    "                full_output = True)\n",
    "\n",
    "            x_post_mode = np.reshape(res[0],[xdim,experiment.T])\n",
    "            y_pred_mode_nrn = np.exp(C_nrn.dot(x_post_mode).T + d_nrn)\n",
    "            pred_err_mode = pred_err_mode # + what needs to change - np.dot(experiment.data[tr]['Y'][nrn]-y_pred_mode_nrn,experiment.data[tr]['Y'][nrn]-y_pred_mode_nrn)\n",
    "            y_pred_mode_tr.append(y_pred_mode_nrn)\n",
    "        y_pred_mode_all.append(y_pred_mode_tr)\n",
    "    y_pred_mode = np.asarray(y_pred_mode_all)\n",
    "    pred_err_mode = pred_err_mode\n",
    "    return y_pred_mode, pred_err_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(util)\n",
    "importlib.reload(engine)\n",
    "importlib.reload(inference)\n",
    "\n",
    "# # your code here\n",
    "cv = cvalidate.crossValidation(data,\n",
    "        numTrainingTrials = 80,\n",
    "        numTestTrials = 20,\n",
    "        maxXdim = 5,\n",
    "        maxEMiter = 3,\n",
    "        batchSize = 5,\n",
    "        inferenceMethod = 'laplace',\n",
    "        learningMethod = 'batch') #, \n",
    "        #errorFunc = predictiveLogLikeErrorFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the test error\n",
    "\n",
    "Make a plot of the test error for the five different models. As a baseline, please also include the test error of a model without a latent variable. This is essentially the mean-squared error of a constant rate model (or Poisson likelihood if you did the optional part above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeTrials(data):\n",
    "    trialsAmt = data.numTrials\n",
    "    neuronsAmt = data.ydim\n",
    "    binsAmt = data.data[0]['Y'].shape[1]\n",
    "    \n",
    "    mergedTrials = np.zeros((trialsAmt, neuronsAmt, binsAmt))\n",
    "    \n",
    "    for t, trial in enumerate(data.data):\n",
    "        mergedTrials[t, :, :] = trial['Y']\n",
    "        \n",
    "    return mergedTrials\n",
    "\n",
    "# SPLIT THE DATA\n",
    "train, test = util.splitTrainingTestDataset(data, numTrainingTrials=80, numTestTrials=20)\n",
    "\n",
    "mergedTrainData = mergeTrials(train)\n",
    "mergedTestData = mergeTrials(test)\n",
    "\n",
    "# FIT\n",
    "fit_0 = np.mean(mergedTrainData, axis=0)  # average over trials and bins\n",
    "\n",
    "# COMPUTE ERROR\n",
    "err_0 = ((fit_0-mergedTestData)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "errs = np.concatenate([[cv.err_0], cv.errs], axis=0)\n",
    "plt.figure(figsize=(5,4))\n",
    "# plt.axhline(errs[0], color='r', markersize=5,linewidth=2, label=\"Baseline\")\n",
    "#plt.plot(np.arange(0, 6), errs,'b.-',markersize=5,linewidth=2)\n",
    "plt.scatter(0, errs[0], color='red', label=\"Baseline (Hacky)\")\n",
    "plt.scatter(0, err_0, color='orange', label=\"Baseline (Mean Squared)\")\n",
    "plt.scatter(np.arange(1, 6), errs[1:])\n",
    "plt.xlabel('Latent Dimensionality')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Latent Dimension vs. Prediction Error')\n",
    "plt.grid(which='both')\n",
    "plt.tight_layout()\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Visualization: population rasters and latent state. Use the model with a single latent state. \n",
    "\n",
    "Create a raster plot where you show for each trial the spikes of all neurons as well as the trajectory of the latent state `x`. Sort the neurons by their weights `c_k`. Plot only the first 20 trials.\n",
    "\n",
    "*Grading: 2 pts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructCMatrix(fits):\n",
    "    C = np.zeros((np.shape(fits[0].optimParams['C'])[0], np.shape(fits)[0]))\n",
    "    for i, fit in enumerate(fits):\n",
    "        C[:, i] = fit.optimParams['C'][:, 0]\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are asked to use a model with a single latent state; in this case, we will just be working with C[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_est_matrix = constructCMatrix(cv.fits)\n",
    "c = C_est_matrix[:, 0]\n",
    "\n",
    "x_fitted = np.array(cv.fits[0].infRes['post_mean']).reshape((80,20))\n",
    "trialDur = data.trialDur\n",
    "numBins = data.binSize\n",
    "bin_length = trialDur/numBins\n",
    "bin_midpoints = np.linspace(0.5*bin_length,trialDur-0.5*bin_length,numBins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuronIndicesSortedByWeight = np.argsort(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_alltrials = np.zeros((20, 55, 20)) #20 trials, 55 neurons, 20 time bins\n",
    "spiketimes_alltrials = [] #np.zeros((20, 55)) #20 trials, 55 neurons, \n",
    "\n",
    "for trial in range(0, 20): \n",
    "    Y = data.data[trial]['Y'][neuronIndicesSortedByWeight]\n",
    "    spiketimes = data.data[trial]['spike_time'][neuronIndicesSortedByWeight]\n",
    "    \n",
    "    Y_alltrials[trial, :, :] = Y \n",
    "    spiketimes_alltrials.append(spiketimes)\n",
    "spiketimes_alltrials = np.array(spiketimes_alltrials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotRaster(trialsAmt, Y_all, spiketimes_all, scale = 1.0, spacing=1.0, flip=False):\n",
    "    if (flip==False):\n",
    "        flip = 1.0\n",
    "    else: \n",
    "        flip = -1.0\n",
    "                \n",
    "    # For each direction \n",
    "    for trial in range(0, 20): \n",
    "        Y = Y_all[trial] \n",
    "        spiketimes = spiketimes_all[trial] \n",
    "        # Use different colors for odd and even \n",
    "        color = \"red\"\n",
    "        alternateColor = \"blue\"\n",
    "        if (trial % 2 == 0): \n",
    "            color = \"blue\"\n",
    "            alternateColor = \"red\"\n",
    "        \n",
    "        normalizedY = Y[:, :] #/ (np.max(Y[:, :]) * 55)\n",
    "        normalizedSpiketimes = spiketimes[:]\n",
    "        plt.axhline(flip*spacing*trial*scale, color='black', markersize=0.1,linewidth=0.2)\n",
    "        for neuron in range(0, 55):\n",
    "            neuronY = normalizedY[neuron, :] / 55 # (np.max(normalizedY[neuron, :]) * 55)\n",
    "            neuronSpiketimes = normalizedSpiketimes[neuron] / 55\n",
    "            y = (flip*(spacing)*trial + flip*neuron/55 + (np.ones((np.shape(neuronSpiketimes)))))*scale\n",
    "            plt.scatter(neuronSpiketimes[:], y, c=color, s = 2) \n",
    "\n",
    "    \n",
    "    yticks = flip*np.arange(0, spacing*(trialsAmt), spacing)*scale\n",
    "    labels = np.array(yticks/(spacing) + 1, dtype=int)\n",
    "    plt.yticks(yticks, labels, fontsize=12)\n",
    "    plt.ylim(0, (trialsAmt)*spacing*scale)\n",
    "#     plt.xlim(-1, 21)\n",
    "#     plt.xticks(np.arange(0, 20))\n",
    "#     plt.xticks(fontsize=12)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Trials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 18))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plotRaster(20, Y_alltrials, spiketimes_alltrials, scale=1.0, spacing=2.0, flip=False)\n",
    "\n",
    "plt.subplot(1, 2, 2) \n",
    "for t in range(0, 20): \n",
    "    plt.plot(cv.fits[0].infRes['post_mean'][t].T, label=sprintf(\"Trial %i\", t+1))\n",
    "\n",
    "plt.title(\"Latent State Trajectories\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.yticks([])\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.fits[0].plotTrajectory();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.fits[0].plotParamSeq();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Visualization of covariance matrix.\n",
    "\n",
    "Plot (a) the noise covariance matrix as well as its approximation using (b) one and (c) five latent variable(s). Use the analytical solution for the covariance matrix of the approximation*. Note that the solution is essentially the mean and covariance of the [log-normal distribution](https://en.wikipedia.org/wiki/Log-normal_distribution).\n",
    "\n",
    "$ \\mu = \\exp(\\frac{1}{2} \\text{ diag}(CC^T)+d)$\n",
    "\n",
    "$ \\text{Cov}= \\mu\\mu^T \\exp(CC^T)+\\text{ diag}(\\mu) - \\mu\\mu^T$ \n",
    "\n",
    "*[Krumin, M., and Shoham, S. (2009). Generation of Spike Trains with Controlled Auto- and Cross-Correlation Functions. Neural Computation 21, 1642–1664](http://www.mitpressjournals.org/doi/10.1162/neco.2009.08-08-847).\n",
    "\n",
    "*Grading: 3 pts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit_1 = cv.fits[0]\n",
    "fit_5 = cv.fits[4]\n",
    "\n",
    "fit_1.performSpikeCountAnalysis()\n",
    "fit_5.performSpikeCountAnalysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Plot noise covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your plot here\n",
    "noise_cov_obs = fit_1.E_yy_obs\n",
    "noise_cov_1 = fit_1.E_yy_optim_params\n",
    "noise_cov_5 = fit_5.E_yy_optim_params\n",
    "\n",
    "vmin = np.min([noise_cov_obs, noise_cov_1, noise_cov_5])\n",
    "vmax = np.max([noise_cov_obs, noise_cov_1, noise_cov_5])\n",
    "vbounds = np.max([np.abs(vmin),np.abs(vmax)])\n",
    "\n",
    "fig = plt.figure(figsize=(20,7))\n",
    "plt.suptitle('Observed & Estimated Noise Covariance Matrices', fontsize=20)\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"Observed Noise Covariance Matrix\", fontsize=15)\n",
    "plt.imshow(noise_cov_obs, vmin=-vbounds, vmax=vbounds)\n",
    "plt.colorbar()\n",
    "plt.tick_params(labelbottom=True, labeltop=False)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Noise Covariance Matrix for 1 Latent Var\", fontsize=15)\n",
    "plt.imshow(noise_cov_1, vmin=-vbounds, vmax=vbounds)\n",
    "plt.colorbar()\n",
    "plt.tick_params(labelbottom=True, labeltop=False)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"Noise Covariance Matrix for 5 Latent Vars\", fontsize=15)\n",
    "plt.imshow(noise_cov_5, vmin=-vbounds, vmax=vbounds)\n",
    "plt.colorbar()\n",
    "plt.tick_params(labelbottom=True, labeltop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # your plot here\n",
    "# diff_obs_1 = np.abs(noise_cov_obs-noise_cov_1)\n",
    "# diff_obs_5 = np.abs(noise_cov_obs-noise_cov_5)\n",
    "# diff_1_5 = np.abs(noise_cov_1-noise_cov_5)\n",
    "\n",
    "# vmin = np.min([diff_obs_1,diff_obs_5,diff_1_5])\n",
    "# vmax = np.max([diff_obs_1,diff_obs_5,diff_1_5])\n",
    "# vbounds = np.max([np.abs(vmin),np.abs(vmax)])\n",
    "\n",
    "# plt.figure(figsize=(20,7))\n",
    "# plt.suptitle('Absolute differences between noise covariance matrices',fontsize=20)\n",
    "\n",
    "# plt.subplot(1,3,1)\n",
    "# plt.imshow(diff_obs_1, vmin=-vbounds, vmax=vbounds)\n",
    "# plt.colorbar()\n",
    "# plt.title('|Observed - 1 latent var|',fontsize=15)\n",
    "# plt.tick_params(labelbottom=True,labeltop=False)\n",
    "\n",
    "# plt.subplot(1,3,2)\n",
    "# plt.imshow(diff_obs_5, vmin=-vbounds, vmax=vbounds)\n",
    "# plt.colorbar()\n",
    "# plt.title('|Observed - 5 latent vars|',fontsize=15)\n",
    "# plt.tick_params(labelbottom=True,labeltop=False)\n",
    "\n",
    "# plt.subplot(1,3,3)\n",
    "# plt.imshow(diff_1_5, vmin=-vbounds, vmax=vbounds)\n",
    "# plt.colorbar()\n",
    "# plt.title('|1 latent variable - 5 latent vars|',fontsize=15)\n",
    "# plt.tick_params(labelbottom=True,labeltop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical Approximations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mu(C,d):\n",
    "    return np.exp((1/2)*np.diag(np.outer(C,C))+d)\n",
    "\n",
    "def noise_cov_approx(C,d):\n",
    "    mean = mu(C,d)\n",
    "    return np.outer(mean,mean) * np.exp(np.outer(C,C))+ np.diag(mean)  - np.outer(mean,mean.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Plot aproximation of noise covariance matrix using 1 latent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_est = C_est_matrix[:, 0]\n",
    "Σ_est = noise_cov_approx(C_est, cv.fits[0].optimParams['d'])\n",
    "plt.title(\"Analytically Approximated Noise Covariance Matrix for 1 Latent Var\", fontsize=15)\n",
    "plt.imshow(Σ_est)\n",
    "plt.colorbar()\n",
    "plt.tick_params(labelbottom=True, labeltop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Plot approximation of noise covariance matrix using 5 latent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_est = C_est_matrix[:, 4]\n",
    "Σ_est = noise_cov_approx(C_est, cv.fits[4].optimParams['d'])\n",
    "plt.title(\"Analytically Approximated Noise Covariance Matrix for 5 Latent Vars\", fontsize=15)\n",
    "plt.imshow(Σ_est)\n",
    "plt.colorbar()\n",
    "plt.tick_params(labelbottom=True, labeltop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nteract": {
   "version": "0.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
