{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 1\n",
    "# Exercises 4\n",
    "\n",
    "# Wy Ming Lin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages, do some initial data cleaning steps listed on the exercise sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import statsmodels.regression.linear_model as se\n",
    "import statsmodels.api as sm\n",
    "import sklearn.model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statistics\n",
    "import sklearn.discriminant_analysis as da\n",
    "from prettytable import PrettyTable\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# this is just to get rid of all the future warnings that are irrelevant\n",
    "# and to make my results easier to see\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# load data\n",
    "datapath = '/Users/wyminglin/Desktop/ML_exercise_data/EEG_data.csv'\n",
    "rawdata = pd.read_csv(datapath)\n",
    "\n",
    "# change SubjectID and VideoID to int\n",
    "rawdata['SubjectID'] = rawdata['SubjectID'].astype('int')\n",
    "rawdata['VideoID'] = rawdata['VideoID'].astype('int')\n",
    "\n",
    "# change predefinedlabel to ExpectedConfusion and user-definedlabel to ReportedConfusion\n",
    "rawdata.columns = rawdata.columns.str.replace('predefinedlabel','ExpectedConfusion')\n",
    "rawdata.columns = rawdata.columns.str.replace('user-definedlabeln','ReportedConfusion')\n",
    "\n",
    "# take the median for each SubjectID and VideoID\n",
    "data = rawdata.copy()\n",
    "data = data.groupby(['SubjectID','VideoID']).median()\n",
    "\n",
    "# create list of predictors for later use\n",
    "predictors = ['Delta', 'Theta', 'Alpha1', 'Alpha2', 'Beta1', 'Beta2', 'Gamma1', 'Gamma2']\n",
    "\n",
    "# initiate lists that will hold all the accuracies and SDs for making the table later\n",
    "means, SDs = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will be doing multiple round of cross validations, I will define a function here that will help make the process easier and pain free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(predictors,target,n_folds,test_type):\n",
    "    \"\"\"\n",
    "    Function to make running analyses in this exercise easier and more compact.\n",
    "    A repeated 10-fold cross validation will be used for each exercise, and the option \n",
    "    to choose among logistic regression, linear discriminant analysis and \n",
    "    quadratic discriminant analysis will be given.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    predictors : array (n_observations, n_predictors)\n",
    "    \n",
    "    target : array (n_observations)\n",
    "    \n",
    "    n_folds : int\n",
    "        number of folds of k-means cross validation\n",
    "        \n",
    "    test_type : str\n",
    "        - log_reg : logistic regression\n",
    "        - linear : linear discriminant analysis\n",
    "        - quadratic: quadratic discriminant analysis\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    test_mean : mean accuracy of the scores of all rounds of tests, rounded to three decimal places\n",
    "    \n",
    "    test_sd : standard deviation of the scores of all rounds of tests, rounded to three decimal places\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # create folds\n",
    "    kf = sklearn.model_selection.RepeatedKFold(n_splits=n_folds,random_state=None)\n",
    "    \n",
    "    # define model\n",
    "    if test_type == 'linear':\n",
    "        model = da.LinearDiscriminantAnalysis()\n",
    "    elif test_type == 'quadratic':\n",
    "        model = da.QuadraticDiscriminantAnalysis()\n",
    "    elif test_type == 'logreg':\n",
    "        model = LogisticRegression()\n",
    "    \n",
    "    # create lists to hold results\n",
    "    train_acc, test_acc = [],[]\n",
    "    \n",
    "    # split the data\n",
    "    for train_index, test_index in kf.split(X=predictors,y=target):\n",
    "        \n",
    "        # gather training set data\n",
    "        training_predictors = predictors.iloc[train_index]\n",
    "        training_targets = target.iloc[train_index]\n",
    "        \n",
    "        # train the data\n",
    "        train_fit = model.fit(training_predictors,training_targets)\n",
    "        \n",
    "        # score the model on the training data\n",
    "        train_acc.append(train_fit.score(training_predictors,training_targets))\n",
    "        \n",
    "        # gather test set data\n",
    "        test_predictors = predictors.iloc[test_index]\n",
    "        test_targets = target.iloc[test_index]\n",
    "        \n",
    "        # score the model on the test data\n",
    "        test_acc.append(model.score(test_predictors,test_targets))\n",
    "        \n",
    "    # calculate mean and sd\n",
    "    mean_test = np.mean(test_acc)\n",
    "    sd_test = statistics.stdev(test_acc)\n",
    "    \n",
    "    print('Accuracy of repeated {}-folds cross validation on test data:'.format(n_folds))\n",
    "    print('\\tmean = {}'.format(mean_test))\n",
    "    print('\\tstandard deviation = {}'.format(sd_test))\n",
    "    \n",
    "\n",
    "    return round(mean_test,3), round(sd_test,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E1. Is there class imbalance for outcome variables ReportedConfusion and ExpectedConfusion? In the rest of the exercises, always use ReportedConfusion as outcome variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    50\n",
      "0.0    50\n",
      "Name: ExpectedConfusion, dtype: int64\n",
      "\n",
      "1.0    51\n",
      "0.0    49\n",
      "Name: ReportedConfusion, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data['ExpectedConfusion'].value_counts())\n",
    "print('')\n",
    "print(data['ReportedConfusion'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both ExpectedConfusion and ReportedConfusion, which are both binary variables that can take either the value 0.0 or 1.0. There is roughly a 50 percent chance to get either one of both values. This shows that both variables are balanced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E2. Run logistic regression with all 100 observations of the 8 continuous predictors, columns 6-13 of the data (Delta to Gamma2). Report training accuracy, i.e. the fraction of observations that are correctly classified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, a logistic regression model will be trained on all 100 observations in the training data and will then be tested on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean training accuracy: 0.66\n"
     ]
    }
   ],
   "source": [
    "# initiate the logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# define the data (data and predictor variables)\n",
    "x_data = data[predictors]\n",
    "y_data = data['ReportedConfusion']\n",
    "\n",
    "# train model on the 100 observations\n",
    "model.fit(x_data,y_data)\n",
    "\n",
    "# score how well the classifier trained on the data\n",
    "E2_score = model.score(x_data,y_data)\n",
    "\n",
    "# append score to means list\n",
    "means.append(E2_score)\n",
    "SDs.append('') # dummy to hold the spot (this is necessary for later when I make the table)\n",
    "\n",
    "print('Mean training accuracy: {}'.format(E2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E3. Run logistic regression with all 100 of the 8 continuous predictors, but use repeated 10-fold cross validation to calculate the prediction accuracy on the test set. Report both the mean and the standard deviation of the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of repeated 10-folds cross validation on test data:\n",
      "\tmean = 0.5990000000000001\n",
      "\tstandard deviation = 0.14248515783063231\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression model with a repeated 10 folds cross validation\n",
    "\n",
    "E3_test_mean, E3_test_sd = run_model(x_data,y_data,10,'logreg')\n",
    "\n",
    "#  append score/sd on means and SDs lists\n",
    "means.append(E3_test_mean)\n",
    "SDs.append(E3_test_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean accuracy for predicting the test data lowered compared to how it did on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E4. Can you improve the prediction accuracy on the test set by reducing the number of predictors? You can get some ideas about which predictors to pick from your analysis of predictor correction in E7 of exercises set 3. Report both the mean and the standard deviation of the prediction accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results of E7 of exercise set 3, we see the following patterns:\n",
    "- Delta, Theta, Alpha1 and Alpha2 are highly correlated with each other\n",
    "- Beta2 and Gamma1 are highly correlated with each other\n",
    "- Beta1 and Gamma2 are stand alone and are not correlated much with the other frequency bands \n",
    "\n",
    "We will therefore include Beta1 and Gamma2 in the model as well as Theta and Beta2 in the model (Theta and Beta2 being representative frequency bands from their highly correlated blocks in the correlation matrix) and run the logistic regression model again based on this reduced set of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of repeated 10-folds cross validation on test data:\n",
      "\tmean = 0.612\n",
      "\tstandard deviation = 0.14652886087971567\n"
     ]
    }
   ],
   "source": [
    "# define the new predictors (Theta / Gamma1) that will be used in the reduced logistic regression\n",
    "predictors_reduced = ['Theta','Beta1','Beta2','Gamma2']\n",
    "x_reduced = data[predictors_reduced]\n",
    "\n",
    "# run logistic regression model with a repeated 10-folds cross validation\n",
    "E4_test_mean, E4_test_sd = run_model(x_reduced,y_data,10,'logreg')\n",
    "\n",
    "# append score/sd to means and SDs lists\n",
    "means.append(E4_test_mean)\n",
    "SDs.append(E4_test_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we take into consideration only Theta, Beta1, Beta2 and Gamma2, we see a slight increase in the mean accuracy of the logistic regression using a repeated 10 folds k-means cross validation when compared to the mean accuracy from E2 (testing with test data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E5. Does skewness correction of the predictors (as you did in E6 of set 3) improve the prediction accuracy on the test set? Run logistic regression with all 100 observations of the 8 continuous predictors, and use repeated 10-fold cross validation to calculate the prediction acccuracy of the test set. Report both the mean and the standard deviation of the prediction accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results from exercise set 3, we will only correct all but the Delta band for skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of repeated 10-folds cross validation on test data:\n",
      "\tmean = 0.6120000000000001\n",
      "\tstandard deviation = 0.13727506854649335\n"
     ]
    }
   ],
   "source": [
    "# create the new data frame\n",
    "data_corrected = data.copy()\n",
    "for fband in predictors[1:]:\n",
    "    bc,_ = stats.boxcox(data[fband])\n",
    "    data_corrected[fband] = bc\n",
    "    \n",
    "x_corrected = data_corrected[predictors]\n",
    "\n",
    "E5_test_mean, E5_test_sd = run_model(x_corrected,y_data,10,'logreg')\n",
    "means.append(E5_test_mean)\n",
    "SDs.append(E5_test_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skewness correction for all predictors except for Delta does not do much to increase the accuracy when compared to the result from E2 (testing on test data) or E3 (testing on a fit with a reduced number of predictors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Linear and quadratic discriminant analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E6. Run both linear and quadratic discriminant analysis with all 100 observations of the 8 continuous predictors and calculate predictive accuracy. Comment on the performance differences between linear and quadratic discriminant and between logistic regression and linear discriminant analysis. Given the mean and standard deviation of the prediction accuracy, which method would you prefer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Discriminant Analysis\n",
      "Accuracy of repeated 10-folds cross validation on test data:\n",
      "\tmean = 0.605\n",
      "\tstandard deviation = 0.15851265762160013\n",
      "\n",
      "Quadratic Linear Analysis\n",
      "Accuracy of repeated 10-folds cross validation on test data:\n",
      "\tmean = 0.6150000000000001\n",
      "\tstandard deviation = 0.15658927811380324\n"
     ]
    }
   ],
   "source": [
    "# first split up into training and test data, use k-means cv with 10 folds\n",
    "\n",
    "# linear discriminant analysis\n",
    "print('Linear Discriminant Analysis')\n",
    "E6lin_test_mean, E6lin_test_sd = run_model(x_data,y_data,10,'linear')\n",
    "means.append(E6lin_test_mean)\n",
    "SDs.append(E6lin_test_sd)\n",
    "print('')\n",
    "print('Quadratic Linear Analysis')\n",
    "# quadratic discriminant analysis\n",
    "E6quad_test_mean, E6quad_test_sd = run_model(x_data,y_data,10,'quadratic')\n",
    "means.append(E6quad_test_mean)\n",
    "SDs.append(E6quad_test_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear and quadratic discriminant analysis models don't really improve the results compared to the logistic regression models run in the previous exercises. It is interesting to note that the mean accuracy of the linear discriminant analysis is slightly lower quadratic linear analysis. \n",
    "\n",
    "This observation shows, that the variability of the observations within each class don't differ much. Consequently quadratic discriminant analysis doesn't provide more accurate classification boundaries. Since quadratic discriminant analysis provides a non-linear quadratic decision boundary, quadratic discriminant analysis only then give better results, when the decision boundary is moderately non-linear. This might not be the case here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E7. Does skewness correction of the predictors (as you did in E6 of set 3) improve the prediction accuracy on the test set? Run both linear and quadratic discriminant analyses with all 100 observations of the 8 continuous predictors, and use repeated 10-fold cross-validation to calculate the prediction accuracy on the test set. Report both the mean and the standard deviation of the prediction accuracy.\n",
    "\n",
    "#### I would also want you to run a generalized mixed-effects model (GLMM) but Python does not have a direct implementation in StatsModels (it does have the fancier Bayesian generalization of GLMMs but that is too advanced for where we are now in the course). Running a GLMM in R, I found that a mixed-effects logistic regression improved predictive accuracy to 0.875. Only the per subject intercept contributed to this increase in performance. We can obtain the same benefit by preprocessing the data and subtract the mean power for each subject in each frequency band. Use this \"baseline corrected\" power to do the following exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Discriminant Analysis\n",
      "Accuracy of repeated 10-folds cross validation on test data:\n",
      "\tmean = 0.6709999999999999\n",
      "\tstandard deviation = 0.13355663119955422\n",
      "\n",
      "Quadratic Discriminant Analysis\n",
      "Accuracy of repeated 10-folds cross validation on test data:\n",
      "\tmean = 0.499\n",
      "\tstandard deviation = 0.16544705277739846\n"
     ]
    }
   ],
   "source": [
    "print('Linear Discriminant Analysis')\n",
    "E7lin_test_mean, E7lin_test_sd = run_model(x_corrected,y_data,10,'linear')\n",
    "means.append(E7lin_test_mean)\n",
    "SDs.append(E7lin_test_sd)\n",
    "print('')\n",
    "print('Quadratic Discriminant Analysis')\n",
    "E7quad_test_mean, E7quad_test_sd = run_model(x_corrected,y_data,10,'quadratic')\n",
    "means.append(E7quad_test_mean)\n",
    "SDs.append(E7quad_test_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When corrected for skewness, the accuracy for linear discriminant analysis model improves a little bit but accuracy for the quadratic discriminant analysis model drops to about chance level. You can't correct the low accuracy with skewness correction, because it is due to other factors, other than just skewed test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Better preprocessing makes a better classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline correcting will be done by subtracting the mean power for each frequency band in each subject as described previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bc_raw = data.copy()\n",
    "data_bc = pd.DataFrame()\n",
    "\n",
    "# apply baseline correction\n",
    "for (SubjectID), subjectdata in data_bc_raw.groupby(('SubjectID')): # first by subject\n",
    "    for i in predictors:\n",
    "        mean = np.mean(subjectdata[i]) # then by predictor\n",
    "        subjectdata[i] = subjectdata[i] - mean\n",
    "    data_bc = pd.concat([data_bc,subjectdata],ignore_index=True)\n",
    "\n",
    "x_data_bc = data_bc[predictors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E8. Using the baseline corrected EEG power, rerun the analysis with (1) logistic repression, (2) linear, and (3) quadratic discriminant analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression on baseline corrected data\n",
      "Accuracy of repeated 10-folds cross validation on test data:\n",
      "\tmean = 0.8810000000000001\n",
      "\tstandard deviation = 0.10607315943024961\n",
      "\n",
      "Linear discriminant analysis on baseline corrected data\n",
      "Accuracy of repeated 10-folds cross validation on test data:\n",
      "\tmean = 0.8910000000000001\n",
      "\tstandard deviation = 0.0911154224158578\n",
      "\n",
      "Quadratic discriminant analysis on baseline corrected data\n",
      "Accuracy of repeated 10-folds cross validation on test data:\n",
      "\tmean = 0.7850000000000001\n",
      "\tstandard deviation = 0.12175069742400792\n"
     ]
    }
   ],
   "source": [
    "print('Logistic regression on baseline corrected data')\n",
    "E8log_test_mean, E8log_test_sd = run_model(x_data_bc,y_data,10,'logreg')\n",
    "means.append(E8log_test_mean)\n",
    "SDs.append(E8log_test_sd)\n",
    "print('')\n",
    "print('Linear discriminant analysis on baseline corrected data')\n",
    "E8lin_test_mean, E8lin_test_sd = run_model(x_data_bc,y_data,10,'linear')\n",
    "means.append(E8lin_test_mean)\n",
    "SDs.append(E8lin_test_sd)\n",
    "print('')\n",
    "print('Quadratic discriminant analysis on baseline corrected data')\n",
    "E8quad_test_mean, E8quad_test_sd = run_model(x_data_bc,y_data,10,'quadratic')\n",
    "means.append(E8quad_test_mean)\n",
    "SDs.append(E8quad_test_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the results above, mean-subtracted baseline correction improves the accuracy provided by the three models. The logistic regression and linear discriminant analysis models show accuracies in the high 80% whereas the quadratic discriminant analysis model gives a considerably lower accuracy. This is again due to few differences in the variability of the observations within each class. For quadratic discriminant analysis predictor variables are not assumed to have common variance. Since quadratic discriminant analysis provides a non-linear quadratic decision boundary, quadratic discriminant analysis may give better results, when the decision boundary is moderately non-linear. This might not be the case here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E9. Report the summary of your analysis in a formatted table, similar to the one I got (I did some extra things, that you are of course free to try too). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For best results, view in full screen\n",
      "\n",
      "+----------+---------------------------------+--------------------------------------------+---------+--------+\n",
      "| Exercise |              Model              |                 Predictors                 | PredAcc | PredSD |\n",
      "+----------+---------------------------------+--------------------------------------------+---------+--------+\n",
      "|    E2    |   logistic regression training  |           all 8 EEG power bands            |   0.66  |        |\n",
      "|    E3    |  logistic regression 10 fold cv |           all 8 EEG power bands            |  0.599  | 0.142  |\n",
      "|    E4    |  logistic regression 10 fold cv | Theta, Beta1, Beta2 and Gamma1 power bands |  0.612  | 0.147  |\n",
      "|    E5    |  logistic regression 10 fold cv |  all 8 EEG power bands skewness-corrected  |  0.612  | 0.137  |\n",
      "|    E6    |   linear discriminant analysis  |           all 8 EEG power bands            |  0.605  | 0.159  |\n",
      "|    E6    | quadratic discriminant analysis |           all 8 EEG power bands            |  0.615  | 0.157  |\n",
      "|    E7    |   linear discriminant analysis  |  all 8 EEG power bands skewness-corrected  |  0.671  | 0.134  |\n",
      "|    E7    | quadratic discriminant analysis |  all 8 EEG power bands skewness-corrected  |  0.499  | 0.165  |\n",
      "|    E8    |       logistic regression       |   all 8 EEG power bands mean subtracted    |  0.881  | 0.106  |\n",
      "|    E8    |   linear discriminant analysis  |   all 8 EEG power bands mean subtracted    |  0.891  | 0.091  |\n",
      "|    E8    | quadratic discriminant analysis |   all 8 EEG power bands mean subtracted    |  0.785  | 0.122  |\n",
      "+----------+---------------------------------+--------------------------------------------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "# PrettyTable will be used to make the formatted table\n",
    "\n",
    "# column names\n",
    "col_names = ['Exercise','Model', 'Predictors', 'PredAcc', 'PredSD']\n",
    "\n",
    "# create a list with all the exercises, models and predictors\n",
    "exercises = ['E2','E3','E4','E5','E6','E6','E7','E7','E8','E8','E8']\n",
    "\n",
    "models_used = ['logistic regression training',\n",
    "               'logistic regression 10 fold cv',\n",
    "               'logistic regression 10 fold cv',\n",
    "               'logistic regression 10 fold cv',\n",
    "               'linear discriminant analysis',\n",
    "               'quadratic discriminant analysis',\n",
    "               'linear discriminant analysis',\n",
    "               'quadratic discriminant analysis',\n",
    "               'logistic regression',\n",
    "               'linear discriminant analysis',\n",
    "               'quadratic discriminant analysis']\n",
    "\n",
    "predictors_used = ['all 8 EEG power bands', \n",
    "                   'all 8 EEG power bands',\n",
    "                   'Theta, Beta1, Beta2 and Gamma1 power bands',\n",
    "                   'all 8 EEG power bands skewness-corrected',\n",
    "                   'all 8 EEG power bands',\n",
    "                   'all 8 EEG power bands',\n",
    "                   'all 8 EEG power bands skewness-corrected',\n",
    "                   'all 8 EEG power bands skewness-corrected',\n",
    "                   'all 8 EEG power bands mean subtracted',\n",
    "                   'all 8 EEG power bands mean subtracted',\n",
    "                   'all 8 EEG power bands mean subtracted']\n",
    "\n",
    "# build table using the means and SDs lists created throughout the script\n",
    "resultsTable = PrettyTable()\n",
    "resultsTable.field_names = col_names\n",
    "for i in range(len(means)):\n",
    "    resultsTable.add_row([exercises[i],models_used[i],predictors_used[i],means[i],SDs[i]])\n",
    "    \n",
    "# print all results!\n",
    "print('For best results, view in full screen\\n')\n",
    "print(resultsTable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
