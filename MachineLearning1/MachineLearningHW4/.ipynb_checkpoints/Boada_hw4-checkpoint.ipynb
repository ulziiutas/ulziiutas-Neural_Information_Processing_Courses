{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises set 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guill\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# SETUP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statistics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics, cross_validation\n",
    "\n",
    "# from scipy import stats\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from IPython.display import Markdown, display\n",
    "\n",
    "# import sklearn as sk\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# import statsmodels.api as sm\n",
    "\n",
    "# from sklearn import datasets, linear_model, metrics\n",
    "# from sklearn.model_selection import train_test_split, KFold, LeaveOneOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read EEG_data.csv into a DataFrame\n",
    "\n",
    "df_raw = pd.read_csv('EEG_data.csv')\n",
    "# df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubjectID               int32\n",
      "VideoID                 int32\n",
      "Attention             float64\n",
      "Mediation             float64\n",
      "Raw                   float64\n",
      "Delta                 float64\n",
      "Theta                 float64\n",
      "Alpha1                float64\n",
      "Alpha2                float64\n",
      "Beta1                 float64\n",
      "Beta2                 float64\n",
      "Gamma1                float64\n",
      "Gamma2                float64\n",
      "predefinedlabel       float64\n",
      "user-definedlabeln    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 2. Change type of \"SubjectID\" and \"VideoID\" to integer\n",
    "\n",
    "df_raw['SubjectID'] = df_raw.SubjectID.astype(int)\n",
    "df_raw['VideoID'] = df_raw.VideoID.astype(int)\n",
    "\n",
    "# View updated types to show the success of the change\n",
    "print(df_raw.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Attention</th>\n",
       "      <th>Mediation</th>\n",
       "      <th>Raw</th>\n",
       "      <th>Delta</th>\n",
       "      <th>Theta</th>\n",
       "      <th>Alpha1</th>\n",
       "      <th>Alpha2</th>\n",
       "      <th>Beta1</th>\n",
       "      <th>Beta2</th>\n",
       "      <th>Gamma1</th>\n",
       "      <th>Gamma2</th>\n",
       "      <th>predefinedlabel</th>\n",
       "      <th>user-definedlabeln</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SubjectID</th>\n",
       "      <th>VideoID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>57.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>304329.0</td>\n",
       "      <td>81382.5</td>\n",
       "      <td>13622.0</td>\n",
       "      <td>14065.5</td>\n",
       "      <td>14312.5</td>\n",
       "      <td>33887.5</td>\n",
       "      <td>29328.5</td>\n",
       "      <td>13104.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>578197.0</td>\n",
       "      <td>109448.0</td>\n",
       "      <td>23867.5</td>\n",
       "      <td>16974.5</td>\n",
       "      <td>18187.0</td>\n",
       "      <td>31988.0</td>\n",
       "      <td>29606.5</td>\n",
       "      <td>10958.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43.5</td>\n",
       "      <td>48.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>463542.0</td>\n",
       "      <td>96455.0</td>\n",
       "      <td>18521.0</td>\n",
       "      <td>13002.0</td>\n",
       "      <td>14092.5</td>\n",
       "      <td>30222.0</td>\n",
       "      <td>27109.0</td>\n",
       "      <td>10977.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>471965.5</td>\n",
       "      <td>64971.0</td>\n",
       "      <td>15899.5</td>\n",
       "      <td>12748.5</td>\n",
       "      <td>11729.0</td>\n",
       "      <td>33487.5</td>\n",
       "      <td>31548.0</td>\n",
       "      <td>11812.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>301557.0</td>\n",
       "      <td>45817.5</td>\n",
       "      <td>14845.5</td>\n",
       "      <td>10775.5</td>\n",
       "      <td>13702.0</td>\n",
       "      <td>31554.0</td>\n",
       "      <td>28606.0</td>\n",
       "      <td>11745.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Attention  Mediation   Raw     Delta     Theta   Alpha1  \\\n",
       "SubjectID VideoID                                                            \n",
       "0         0             57.0       53.0  41.0  304329.0   81382.5  13622.0   \n",
       "          1             47.0       50.0  33.0  578197.0  109448.0  23867.5   \n",
       "          2             43.5       48.0   7.5  463542.0   96455.0  18521.0   \n",
       "          3             52.0       53.0  37.0  471965.5   64971.0  15899.5   \n",
       "          4             53.0       47.0  28.0  301557.0   45817.5  14845.5   \n",
       "\n",
       "                    Alpha2    Beta1    Beta2   Gamma1   Gamma2  \\\n",
       "SubjectID VideoID                                                \n",
       "0         0        14065.5  14312.5  33887.5  29328.5  13104.5   \n",
       "          1        16974.5  18187.0  31988.0  29606.5  10958.5   \n",
       "          2        13002.0  14092.5  30222.0  27109.0  10977.5   \n",
       "          3        12748.5  11729.0  33487.5  31548.0  11812.5   \n",
       "          4        10775.5  13702.0  31554.0  28606.0  11745.5   \n",
       "\n",
       "                   predefinedlabel  user-definedlabeln  \n",
       "SubjectID VideoID                                       \n",
       "0         0                    0.0                 0.0  \n",
       "          1                    0.0                 1.0  \n",
       "          2                    0.0                 1.0  \n",
       "          3                    0.0                 0.0  \n",
       "          4                    0.0                 0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Group by 'SubjectID' and 'VideoID' taking the median\n",
    "\n",
    "df = df_raw.groupby(['SubjectID','VideoID']).median()\n",
    "df.head() # show the summarised DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention            float64\n",
      "Mediation            float64\n",
      "Raw                  float64\n",
      "Delta                float64\n",
      "Theta                float64\n",
      "Alpha1               float64\n",
      "Alpha2               float64\n",
      "Beta1                float64\n",
      "Beta2                float64\n",
      "Gamma1               float64\n",
      "Gamma2               float64\n",
      "ExpectedConfusion      int32\n",
      "ReportedConfusion      int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 4. Rename and change type of 'predefinedlabel' and 'user-definedlabeln'\n",
    "\n",
    "# Rename columns 'predefinedlabel' and 'user-definedlabeln'\n",
    "df = df.rename(columns={'predefinedlabel': 'ExpectedConfusion', 'user-definedlabeln': 'ReportedConfusion'})\n",
    "\n",
    "# Change type of \"SubjectID\" and \"VideoID\" to integer\n",
    "df['ExpectedConfusion'] = df.ExpectedConfusion.astype(int)\n",
    "df['ReportedConfusion'] = df.ReportedConfusion.astype(int)\n",
    "\n",
    "# View updated types to show the success of the change\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ExpectedConfusion</th>\n",
       "      <th>ReportedConfusion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SubjectID</th>\n",
       "      <th>VideoID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ExpectedConfusion  ReportedConfusion\n",
       "SubjectID VideoID                                      \n",
       "0         0                        0                  0\n",
       "          1                        0                  1\n",
       "          2                        0                  1\n",
       "          3                        0                  0\n",
       "          4                        0                  0\n",
       "          5                        1                  1\n",
       "          6                        1                  1\n",
       "          7                        1                  0\n",
       "          8                        1                  1\n",
       "          9                        1                  0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore the response variables 'ExpectedConfusion' and 'ReportedConfusion'\n",
    "targets = ['ExpectedConfusion','ReportedConfusion']\n",
    "df[targets].iloc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'ExpectedConfusion' is predefined for each video:\n",
    "\n",
    "Videos 0 - 4 are clear, 'ExpectedConfusion' = 0 \n",
    "\n",
    "Videos 5 - 9 are confusing, 'ExpectedConfusion' = 1\n",
    "\n",
    "The 'ReportedConfusion' is given by the student after watching the video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there imbalance for 'ExpectedConfusion' and 'ReportedConfusion' ?\n",
    "\n",
    "As 'ExpectedConfusion' is predefined depending on the video and half of the videos (from 0 to 4) are clear and the other half (from 5 to 9) are tagged as confussion, this outcome variable is perfectly balances, i.e. 50:50\n",
    "\n",
    "As 'ReportedConfusion' is given by the subjects, it may be umbalanced, let's check it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50 confusing videos out of 100 recordings.\n",
      "There are 51 confusing videos out of 100 recordings.\n"
     ]
    }
   ],
   "source": [
    "# Recheck that 'ExpectedConfusion' is balanced\n",
    "expected_ones = df['ExpectedConfusion'].sum()\n",
    "print('There are {} confusing videos out of 100 recordings.'.format(expected_ones))\n",
    "# Check if 'ReportedConfusion' is balanced\n",
    "reported_ones = df['ReportedConfusion'].sum()\n",
    "print('There are {} confusing videos out of 100 recordings.'.format(reported_ones))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    51\n",
       "0    49\n",
       "Name: ReportedConfusion, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A useful command to do the same is .value_counts()\n",
    "df['ReportedConfusion'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for ExpectedConfusion is luckly very balanced!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the target \n",
    "target = 'ReportedConfusion'\n",
    "\n",
    "# Select the 8 predictors (power bands)\n",
    "predictors = df.columns[3:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all parameters not specified are set to their defaults\n",
    "logisticRegr = LogisticRegression()\n",
    "\n",
    "# Model is learning the relationship between digits (x_train) and labels (y_train)\n",
    "logisticRegr.fit(df[predictors], df[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we trained our logistic regression, we can make predictions using the testing data to determine the training accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction of a single observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-f8c616223176>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# We take one row from the test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "# We take one row from the test set\n",
    "x_test.iloc[[6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict its outcome\n",
    "logisticRegr.predict(x_test.iloc[[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SubjectID  VideoID\n",
       "1          6          0\n",
       "Name: ReportedConfusion, dtype: int32"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check its real outcome\n",
    "y_test.iloc[[6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction for this observation is correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction of a all observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict its outcome\n",
    "logisticRegr.predict(df[predictors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SubjectID  VideoID\n",
       "0          0          0\n",
       "           1          1\n",
       "           2          1\n",
       "           3          0\n",
       "           4          0\n",
       "           5          1\n",
       "           6          1\n",
       "           7          0\n",
       "           8          1\n",
       "           9          0\n",
       "1          0          0\n",
       "           1          1\n",
       "           2          1\n",
       "           3          1\n",
       "           4          1\n",
       "           5          0\n",
       "           6          0\n",
       "           7          0\n",
       "           8          0\n",
       "           9          0\n",
       "2          0          0\n",
       "           1          1\n",
       "           2          0\n",
       "           3          0\n",
       "           4          1\n",
       "           5          1\n",
       "           6          0\n",
       "           7          1\n",
       "           8          1\n",
       "           9          0\n",
       "                     ..\n",
       "7          0          1\n",
       "           1          1\n",
       "           2          0\n",
       "           3          1\n",
       "           4          1\n",
       "           5          1\n",
       "           6          0\n",
       "           7          1\n",
       "           8          0\n",
       "           9          0\n",
       "8          0          0\n",
       "           1          0\n",
       "           2          0\n",
       "           3          0\n",
       "           4          1\n",
       "           5          1\n",
       "           6          0\n",
       "           7          0\n",
       "           8          1\n",
       "           9          1\n",
       "9          0          1\n",
       "           1          0\n",
       "           2          0\n",
       "           3          1\n",
       "           4          1\n",
       "           5          1\n",
       "           6          0\n",
       "           7          0\n",
       "           8          1\n",
       "           9          0\n",
       "Name: ReportedConfusion, Length: 100, dtype: int32"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check their real outcome\n",
    "df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of them coincide, but not all. With this we can determine the Training Accuracy.\n",
    "\n",
    "$TrainingAccuracy = \\frac{nº of correct predictions}{nº of predictions}$\n",
    "\n",
    "The fraction of observations that are well-classified will give the training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.66\n"
     ]
    }
   ],
   "source": [
    "# The command logisticRegr.score(x_test, y_test) computes it for us\n",
    "accuracy = logisticRegr.score(df[predictors], df[target])\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is predicts shit!!!!! :D Because it only predicts correctly 66% (TRAINING ACCURACY)of the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E3.\n",
    "Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for REPEATED CROSS-VALIDATION\n",
    "\n",
    "def model_repeatedKfolds(predictors, target, K, n_repeats, model_type, result_type):\n",
    "    \"\"\"    \n",
    "    Parameters\n",
    "    ----------\n",
    "    predictors : array (n_observations, n_predictors)\n",
    "\n",
    "    target : array (n_observations)\n",
    "\n",
    "    K : int\n",
    "        number of folds of k-means cross validation\n",
    "\n",
    "    model_type : str\n",
    "        - logreg : logistic regression\n",
    "        - linear : linear discriminant analysis\n",
    "        - quadratic: quadratic discriminant analysis\n",
    "\n",
    "    result_type : str\n",
    "        Displays what kind of results to display\n",
    "        - train : displays mean training accuracy and standard deviation\n",
    "        - test : displays mean test accuracy and standard deviation\n",
    "    \"\"\"\n",
    "        \n",
    "    # split the data in all possible folds\n",
    "    kf = RepeatedKFold(n_splits=K, n_repeats=n_repeats)\n",
    "        \n",
    "    # define model\n",
    "    if model_type == 'linear':\n",
    "        model = da.LinearDiscriminantAnalysis()\n",
    "    elif model_type == 'quadratic':\n",
    "        model = da.QuadraticDiscriminantAnalysis()\n",
    "    elif model_type == 'logreg':\n",
    "        model = LogisticRegression()\n",
    "\n",
    "    # initialize lists to store results\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "\n",
    "    # split the data\n",
    "    for train_index, test_index in kf.split(X=predictors,y=target):\n",
    "\n",
    "#       print('TRAIN:', train_index, 'TEST:', test_index)\n",
    "\n",
    "         # gather training set data\n",
    "        training_predictors = predictors.iloc[train_index]\n",
    "        training_targets = target.iloc[train_index]\n",
    "\n",
    "        # train the data\n",
    "        train_fit = model.fit(training_predictors,training_targets)\n",
    "\n",
    "        # score the model on the training data\n",
    "        train_acc.append(train_fit.score(training_predictors,training_targets))\n",
    "\n",
    "        # gather test set data\n",
    "        test_predictors = predictors.iloc[test_index]\n",
    "        test_targets = target.iloc[test_index]\n",
    "\n",
    "        # score the model on the test data\n",
    "        test_acc.append(model.score(test_predictors,test_targets))\n",
    "\n",
    "    # calculate mean and sd\n",
    "    mean_train = np.mean(train_acc)\n",
    "    sd_train = statistics.stdev(train_acc)\n",
    "    mean_test = np.mean(test_acc)\n",
    "    sd_test = statistics.stdev(test_acc)\n",
    "\n",
    "    if result_type == 'on_trainset':\n",
    "        print('Accuracy of k-means cross validation with {} folds on training data:'.format(K))\n",
    "        print('\\tmean = {}'.format(mean_train))\n",
    "        print('\\tstandard deviation = {}'.format(sd_train))\n",
    "    if result_type == 'on_testset':\n",
    "        print('Accuracy of k-means cross validation with {} folds on test data:'.format(K))\n",
    "        print('\\tmean = {}'.format(mean_test))\n",
    "        print('\\tstandard deviation = {}'.format(sd_test))\n",
    "    if result_type == 'on_both':\n",
    "        print('Accuracy of k-means cross validation with {} folds on training data:'.format(K))\n",
    "        print('\\tmean = {}'.format(mean_train))\n",
    "        print('\\tstandard deviation = {}'.format(sd_train))\n",
    "        print('')\n",
    "        print('Accuracy of k-means cross validation with {} folds on test data:'.format(K))\n",
    "        print('\\tmean = {}'.format(mean_test))\n",
    "        print('\\tstandard deviation = {}'.format(sd_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have 100 observations and divide them in 10 folds, for each iteration we'll have 90 observations for training and 10 for testing.\n",
    "\n",
    "Now we are ready to run the logistic regression for each iteration: But WyMing wrote a functions that makes everything for me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of k-means cross validation with 10 folds on training data:\n",
      "\tmean = 0.6626666666666666\n",
      "\tstandard deviation = 0.024695173667018316\n"
     ]
    }
   ],
   "source": [
    "model_repeatedKfolds(predictors=df[predictors], target=df[target], K=10, n_repeats=10, model_type='logreg', result_type='on_trainset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each iteration: [0.63636364 0.7        0.7        0.7        0.7        0.5\n",
      " 0.7        0.6        0.6        0.55555556]\n",
      "Mean: 0.6391919191919191\n",
      "SD: 0.06923695356679323\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "accuracy_iterations = cross_val_score(logisticRegr, X=df[predictors], y=df[target] , cv=10 ,scoring='accuracy')\n",
    "print ('Accuracy for each iteration: {}'.format(accuracy_iterations))\n",
    "print ('Mean: {}'.format(accuracy_iterations.mean()))\n",
    "print ('SD: {}'.format(accuracy_iterations.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we just computed the accuracy for one split. Now with cross-validation, we computed it for 10 different possible splittings. Thanks to this we got a more conclusive value for the accuracy, which is the mean of the accuracy of each iterations, 63%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  E4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From hw3 E7, how should the heatmap help me choosing which predictor should I drop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Theta', 'Alpha1', 'Alpha2', 'Beta1', 'Beta2', 'Gamma1', 'Gamma2']"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose the predictors that you want to use\n",
    "predictors_new = df.columns[3:11]\n",
    "predictors_new = ['Theta', 'Alpha1', 'Alpha2', 'Beta1', 'Beta2', 'Gamma1', 'Gamma2']\n",
    "predictors_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each iteration: [0.63636364 0.7        0.7        0.6        0.7        0.4\n",
      " 0.7        0.6        0.5        0.44444444]\n",
      "Mean: 0.5980808080808081\n",
      "SD: 0.10746302244844845\n"
     ]
    }
   ],
   "source": [
    "accuracy_iterations_new = cross_val_score(logisticRegr, X=df[predictors_new], y=df[target] , cv=10 ,scoring='accuracy')\n",
    "print ('Accuracy for each iteration: {}'.format(accuracy_iterations_new))\n",
    "print ('Mean: {}'.format(accuracy_iterations_new.mean()))\n",
    "print ('SD: {}'.format(accuracy_iterations_new.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying different combinations of predictors, we observed that dropping alpha1 and alpha2 gives a better accuracy:\n",
    "\n",
    "Dropping alpha1 and alpha2:\n",
    "\n",
    "Mean: 0.6584848484848485\n",
    "\n",
    "SD: 0.09372458333148866\n",
    "\n",
    "\n",
    "When dropping other predictors like Delta, the accuracy gets worse\n",
    "\n",
    "Dropping delta:\n",
    "\n",
    "Mean: 0.6173737373737372\n",
    "\n",
    "SD: 0.12106202828458776\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTIONS:\n",
    "\n",
    "Is it really correct to compute accuracy using the training said? We are asked to do so in E2 because we use all the data for training and then we're not left with any data to test."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
